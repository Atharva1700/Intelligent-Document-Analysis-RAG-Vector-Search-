# ─────────────────────────────────────────────────────────────────
# RAG System Configuration
# Copy this file to .env and fill in your values
# ─────────────────────────────────────────────────────────────────

# ─── Vector Store ─────────────────────────────────────────────────
CHROMA_PERSIST_DIR=./chroma_db
COLLECTION_NAME=rag_documents

# ─── Embedding Model ──────────────────────────────────────────────
# Options: bge-small | bge-base | bge-large | gte-base | minilm
# bge-base is the best balance of speed and quality
EMBEDDING_MODEL=bge-base

# ─── Document Processing ──────────────────────────────────────────
CHUNK_SIZE=512          # Tokens per chunk
CHUNK_OVERLAP=128       # Overlap between chunks (context preservation)

# ─── Retrieval ────────────────────────────────────────────────────
TOP_K=5                 # Final number of chunks to pass to LLM
USE_RERANKER=true       # Enable cross-encoder reranking (+accuracy, slight latency)

# ─── LLM Provider ─────────────────────────────────────────────────
# Options: ollama | openai | anthropic | google | groq

# --- Option A: Ollama (FREE, local, no API key needed) ---
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2
# Install Ollama: https://ollama.ai → then run: ollama pull llama3.2

# --- Option B: OpenAI ---
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# LLM_API_KEY=sk-...

# --- Option C: Anthropic Claude ---
# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-3-haiku-20240307
# LLM_API_KEY=sk-ant-...

# --- Option D: Google Gemini ---
# LLM_PROVIDER=google
# LLM_MODEL=gemini-1.5-flash
# LLM_API_KEY=AIza...

# --- Option E: Groq (ultra-fast, free tier) ---
# LLM_PROVIDER=groq
# LLM_MODEL=llama-3.1-8b-instant
# LLM_API_KEY=gsk_...
